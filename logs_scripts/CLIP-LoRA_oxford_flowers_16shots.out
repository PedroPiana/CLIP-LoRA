created virtual environment CPython3.10.13.final.0-64 in 564ms
  creator CPython3Posix(dest=/localscratch/pedro36.56834460.0/env, clear=False, no_vcs_ignore=False, global=False)
  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/home/pedro36/.local/share/virtualenv)
    added seed packages: pip==25.1.1, setuptools==80.7.1, wheel==0.45.1+computecanada
  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v4, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already satisfied: pip in /localscratch/pedro36.56834460.0/env/lib/python3.10/site-packages (25.1.1)
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v4, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/torch-2.6.0+computecanada-cp310-cp310-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/torchvision-0.21.0+computecanada-cp310-cp310-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/torchaudio-2.6.0+computecanada-cp310-cp310-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/ftfy-6.3.0+computecanada-py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/scipy-1.15.1+computecanada-cp310-cp310-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/regex-2024.9.11+computecanada-cp310-cp310-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/tqdm-4.67.1+computecanada-py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/gdown-5.2.0+computecanada-py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/pandas-2.2.3+computecanada-cp310-cp310-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/filelock-3.18.0+computecanada-py3-none-any.whl (from torch)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/typing_extensions-4.14.0+computecanada-py3-none-any.whl (from torch)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/networkx-3.5+computecanada-py3-none-any.whl (from torch)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/jinja2-3.1.6+computecanada-py3-none-any.whl (from torch)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/fsspec-2025.5.1+computecanada-py3-none-any.whl (from torch)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/sympy-1.13.1+computecanada-py3-none-any.whl (from torch)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/mpmath-1.3.0+computecanada-py3-none-any.whl (from sympy==1.13.1->torch)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/numpy-2.2.2+computecanada-cp310-cp310-linux_x86_64.whl (from torchvision)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/Pillow_SIMD-9.5.0.post2+computecanada-cp310-cp310-linux_x86_64.whl (from torchvision)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/wcwidth-0.2.13+computecanada-py2.py3-none-any.whl (from ftfy)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/beautifulsoup4-4.13.4+computecanada-py3-none-any.whl (from gdown)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/requests-2.32.4+computecanada-py3-none-any.whl (from gdown)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/python_dateutil-2.9.0.post0+computecanada-py2.py3-none-any.whl (from pandas)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/pytz-2025.2+computecanada-py2.py3-none-any.whl (from pandas)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/tzdata-2025.2+computecanada-py2.py3-none-any.whl (from pandas)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/six-1.17.0+computecanada-py2.py3-none-any.whl (from python-dateutil>=2.8.2->pandas)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/soupsieve-2.7+computecanada-py3-none-any.whl (from beautifulsoup4->gdown)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/MarkupSafe-2.1.5+computecanada-cp310-cp310-linux_x86_64.whl (from jinja2->torch)
INFO: pip is looking at multiple versions of networkx to determine which version is compatible with other requirements. This could take a while.
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/networkx-3.4.2+computecanada-py3-none-any.whl (from torch)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/charset_normalizer-3.4.2+computecanada-py3-none-any.whl (from requests[socks]->gdown)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/idna-3.10+computecanada-py3-none-any.whl (from requests[socks]->gdown)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/urllib3-2.5.0+computecanada-py3-none-any.whl (from requests[socks]->gdown)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/certifi-2025.6.15+computecanada-py3-none-any.whl (from requests[socks]->gdown)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/PySocks-1.7.1+computecanada-py3-none-any.whl (from requests[socks]->gdown)
Installing collected packages: wcwidth, pytz, mpmath, urllib3, tzdata, typing-extensions, tqdm, sympy, soupsieve, six, regex, PySocks, pillow-simd, numpy, networkx, MarkupSafe, idna, ftfy, fsspec, filelock, charset-normalizer, certifi, scipy, requests, python-dateutil, jinja2, beautifulsoup4, torch, pandas, torchvision, torchaudio, gdown

Successfully installed MarkupSafe-2.1.5+computecanada PySocks-1.7.1+computecanada beautifulsoup4-4.13.4+computecanada certifi-2025.6.15+computecanada charset-normalizer-3.4.2+computecanada filelock-3.18.0+computecanada fsspec-2025.5.1+computecanada ftfy-6.3.0+computecanada gdown-5.2.0+computecanada idna-3.10+computecanada jinja2-3.1.6+computecanada mpmath-1.3.0+computecanada networkx-3.4.2+computecanada numpy-2.2.2+computecanada pandas-2.2.3+computecanada pillow-simd-9.5.0.post2+computecanada python-dateutil-2.9.0.post0+computecanada pytz-2025.2+computecanada regex-2024.9.11+computecanada requests-2.32.4+computecanada scipy-1.15.1+computecanada six-1.17.0+computecanada soupsieve-2.7+computecanada sympy-1.13.1+computecanada torch-2.6.0+computecanada torchaudio-2.6.0+computecanada torchvision-0.21.0+computecanada tqdm-4.67.1+computecanada typing-extensions-4.14.0+computecanada tzdata-2025.2+computecanada urllib3-2.5.0+computecanada wcwidth-0.2.13+computecanada
Preparing dataset.
Reading split from /home/pedro36/projects/def-leszek/pedro36/datasets/DATA/Flower102/split_zhou_OxfordFlowers.json
Creating a 4-shot dataset
Creating a 16-shot dataset

Getting textual features as CLIP's classifier.

Loading visual features and labels from val set.

Loading visual features and labels from test set.

**** Zero-shot CLIP's test accuracy: 67.36. ****

Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
LR: 0.000200, Acc: 62.6225, Loss: 1.8044
LR: 0.000200, Acc: 67.6471, Loss: 1.2741
LR: 0.000200, Acc: 74.1422, Loss: 0.9693
LR: 0.000200, Acc: 81.1275, Loss: 0.7116
LR: 0.000200, Acc: 84.2525, Loss: 0.5630
LR: 0.000199, Acc: 86.8873, Loss: 0.4719
LR: 0.000199, Acc: 89.0931, Loss: 0.3861
LR: 0.000199, Acc: 88.9093, Loss: 0.4265
LR: 0.000198, Acc: 91.6054, Loss: 0.3136
LR: 0.000198, Acc: 92.0956, Loss: 0.2970
LR: 0.000198, Acc: 91.9730, Loss: 0.2828
LR: 0.000197, Acc: 92.5858, Loss: 0.2771
LR: 0.000197, Acc: 93.8113, Loss: 0.2392
LR: 0.000196, Acc: 93.0760, Loss: 0.2683
LR: 0.000196, Acc: 94.1789, Loss: 0.2265
LR: 0.000195, Acc: 94.6691, Loss: 0.1896
LR: 0.000194, Acc: 94.4240, Loss: 0.2138
LR: 0.000194, Acc: 94.9755, Loss: 0.1764
LR: 0.000193, Acc: 94.7917, Loss: 0.1888
LR: 0.000192, Acc: 94.8529, Loss: 0.1904
LR: 0.000191, Acc: 95.0368, Loss: 0.1885
LR: 0.000190, Acc: 95.4044, Loss: 0.1802
LR: 0.000190, Acc: 95.7721, Loss: 0.1495
LR: 0.000189, Acc: 95.7108, Loss: 0.1718
LR: 0.000188, Acc: 94.8529, Loss: 0.1900
LR: 0.000187, Acc: 96.2010, Loss: 0.1477
LR: 0.000186, Acc: 94.9755, Loss: 0.1872
LR: 0.000185, Acc: 95.8333, Loss: 0.1489
LR: 0.000184, Acc: 96.6299, Loss: 0.1388
LR: 0.000183, Acc: 96.3848, Loss: 0.1346
LR: 0.000181, Acc: 96.6299, Loss: 0.1330
LR: 0.000180, Acc: 95.0368, Loss: 0.1857
LR: 0.000179, Acc: 96.3848, Loss: 0.1412
LR: 0.000178, Acc: 96.8137, Loss: 0.1235
LR: 0.000177, Acc: 96.5074, Loss: 0.1258
LR: 0.000175, Acc: 95.8333, Loss: 0.1441
LR: 0.000174, Acc: 96.7525, Loss: 0.1203
LR: 0.000173, Acc: 96.8750, Loss: 0.1179
LR: 0.000171, Acc: 96.6912, Loss: 0.1255
LR: 0.000170, Acc: 97.4265, Loss: 0.0945
LR: 0.000168, Acc: 97.4877, Loss: 0.1055
LR: 0.000167, Acc: 96.9363, Loss: 0.1029
LR: 0.000165, Acc: 96.9975, Loss: 0.1010
LR: 0.000164, Acc: 96.1397, Loss: 0.1288
LR: 0.000162, Acc: 96.8750, Loss: 0.1073
LR: 0.000161, Acc: 97.2426, Loss: 0.0937
LR: 0.000159, Acc: 96.6299, Loss: 0.1278
LR: 0.000157, Acc: 97.3039, Loss: 0.0946
LR: 0.000156, Acc: 97.5490, Loss: 0.0915
LR: 0.000154, Acc: 96.9363, Loss: 0.1048
LR: 0.000152, Acc: 97.3652, Loss: 0.0989
LR: 0.000151, Acc: 97.3039, Loss: 0.1020
LR: 0.000149, Acc: 97.0588, Loss: 0.1035
LR: 0.000147, Acc: 97.2426, Loss: 0.1042
LR: 0.000145, Acc: 96.3848, Loss: 0.1370
LR: 0.000144, Acc: 97.1814, Loss: 0.1189
LR: 0.000142, Acc: 98.0392, Loss: 0.0767
LR: 0.000140, Acc: 97.5490, Loss: 0.1019
LR: 0.000138, Acc: 97.9779, Loss: 0.0804
LR: 0.000136, Acc: 97.9779, Loss: 0.0874
LR: 0.000135, Acc: 97.3652, Loss: 0.1033
LR: 0.000133, Acc: 97.8554, Loss: 0.0791
LR: 0.000131, Acc: 97.9167, Loss: 0.0762
LR: 0.000129, Acc: 96.6912, Loss: 0.1100
LR: 0.000127, Acc: 97.1814, Loss: 0.1072
LR: 0.000125, Acc: 97.7328, Loss: 0.0761
LR: 0.000123, Acc: 97.5490, Loss: 0.0900
LR: 0.000121, Acc: 97.6103, Loss: 0.0835
LR: 0.000119, Acc: 96.3235, Loss: 0.1193
LR: 0.000117, Acc: 97.6103, Loss: 0.0784
LR: 0.000115, Acc: 97.9779, Loss: 0.0765
LR: 0.000113, Acc: 97.9167, Loss: 0.0790
LR: 0.000111, Acc: 97.4877, Loss: 0.0916
LR: 0.000109, Acc: 97.7941, Loss: 0.0827
LR: 0.000107, Acc: 97.5490, Loss: 0.0917
LR: 0.000105, Acc: 98.2230, Loss: 0.0742
LR: 0.000103, Acc: 97.6103, Loss: 0.0809
LR: 0.000101, Acc: 97.6716, Loss: 0.0818
LR: 0.000099, Acc: 98.6520, Loss: 0.0564
LR: 0.000097, Acc: 98.0392, Loss: 0.0654
LR: 0.000095, Acc: 97.6103, Loss: 0.0882
LR: 0.000093, Acc: 98.2843, Loss: 0.0617
LR: 0.000091, Acc: 97.9167, Loss: 0.0692
LR: 0.000089, Acc: 98.3456, Loss: 0.0620
LR: 0.000087, Acc: 97.8554, Loss: 0.0746
LR: 0.000085, Acc: 97.7328, Loss: 0.0813
LR: 0.000084, Acc: 98.2843, Loss: 0.0798
LR: 0.000082, Acc: 97.4877, Loss: 0.0773
LR: 0.000080, Acc: 98.2230, Loss: 0.0634
LR: 0.000078, Acc: 97.9779, Loss: 0.0782
LR: 0.000076, Acc: 98.2230, Loss: 0.0669
LR: 0.000074, Acc: 98.4681, Loss: 0.0567
LR: 0.000072, Acc: 97.9167, Loss: 0.0703
LR: 0.000070, Acc: 98.6520, Loss: 0.0575
LR: 0.000068, Acc: 98.1005, Loss: 0.0666
LR: 0.000066, Acc: 98.2230, Loss: 0.0777
LR: 0.000064, Acc: 98.2230, Loss: 0.0769
LR: 0.000062, Acc: 98.1618, Loss: 0.0731
LR: 0.000061, Acc: 98.2230, Loss: 0.0685
LR: 0.000059, Acc: 97.6103, Loss: 0.0880
LR: 0.000057, Acc: 98.6520, Loss: 0.0558
LR: 0.000055, Acc: 98.4069, Loss: 0.0596
LR: 0.000053, Acc: 97.8554, Loss: 0.0745
LR: 0.000052, Acc: 98.3456, Loss: 0.0640
LR: 0.000050, Acc: 98.0392, Loss: 0.0630
LR: 0.000048, Acc: 98.4069, Loss: 0.0600
LR: 0.000047, Acc: 98.4069, Loss: 0.0579
LR: 0.000045, Acc: 98.5907, Loss: 0.0512
LR: 0.000043, Acc: 97.9167, Loss: 0.0756
LR: 0.000042, Acc: 98.0392, Loss: 0.0635
LR: 0.000040, Acc: 98.5907, Loss: 0.0572
LR: 0.000039, Acc: 98.7132, Loss: 0.0497
LR: 0.000037, Acc: 97.9167, Loss: 0.0707
LR: 0.000035, Acc: 97.9779, Loss: 0.0681
LR: 0.000034, Acc: 98.4681, Loss: 0.0570
LR: 0.000033, Acc: 97.7941, Loss: 0.0682
LR: 0.000031, Acc: 97.9167, Loss: 0.0695
LR: 0.000030, Acc: 98.3456, Loss: 0.0651
LR: 0.000028, Acc: 98.0392, Loss: 0.0666
LR: 0.000027, Acc: 98.5294, Loss: 0.0678
LR: 0.000026, Acc: 98.2843, Loss: 0.0599
LR: 0.000024, Acc: 98.6520, Loss: 0.0521
LR: 0.000023, Acc: 98.4681, Loss: 0.0719
LR: 0.000022, Acc: 98.6520, Loss: 0.0508
LR: 0.000021, Acc: 98.8971, Loss: 0.0392
LR: 0.000019, Acc: 98.4681, Loss: 0.0521
LR: 0.000018, Acc: 98.4681, Loss: 0.0564
LR: 0.000017, Acc: 98.6520, Loss: 0.0562
LR: 0.000016, Acc: 98.2230, Loss: 0.0617
LR: 0.000015, Acc: 98.4681, Loss: 0.0515
LR: 0.000014, Acc: 98.2843, Loss: 0.0591
LR: 0.000013, Acc: 98.4681, Loss: 0.0562
LR: 0.000012, Acc: 98.2843, Loss: 0.0623
LR: 0.000011, Acc: 98.7132, Loss: 0.0554
LR: 0.000010, Acc: 98.5294, Loss: 0.0571
LR: 0.000010, Acc: 98.3456, Loss: 0.0561
LR: 0.000009, Acc: 98.1618, Loss: 0.0685
LR: 0.000008, Acc: 98.4681, Loss: 0.0653
LR: 0.000007, Acc: 98.3456, Loss: 0.0613
LR: 0.000007, Acc: 98.1005, Loss: 0.0637
LR: 0.000006, Acc: 98.5907, Loss: 0.0495
LR: 0.000005, Acc: 98.7745, Loss: 0.0492
LR: 0.000005, Acc: 98.2230, Loss: 0.0622
LR: 0.000004, Acc: 98.4069, Loss: 0.0664
LR: 0.000004, Acc: 97.9167, Loss: 0.0766
LR: 0.000003, Acc: 98.1618, Loss: 0.0560
LR: 0.000003, Acc: 98.0392, Loss: 0.0697
LR: 0.000003, Acc: 98.4069, Loss: 0.0517
LR: 0.000002, Acc: 98.0392, Loss: 0.0739
LR: 0.000002, Acc: 98.4681, Loss: 0.0468
LR: 0.000002, Acc: 98.4069, Loss: 0.0638
LR: 0.000001, Acc: 98.4681, Loss: 0.0617
LR: 0.000001, Acc: 98.5294, Loss: 0.0578
LR: 0.000001, Acc: 98.8358, Loss: 0.0546
LR: 0.000001, Acc: 98.7745, Loss: 0.0459
LR: 0.000001, Acc: 98.8971, Loss: 0.0447
**** Final test accuracy: 98.13. ****

LoRA weights saved to weights/vitb16/oxford_flowers/16shots/seed1/CLIP-LoRA_oxford_flowers.pt
